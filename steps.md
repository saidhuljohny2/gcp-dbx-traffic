# ðŸš¦ GCP Databricks Project

---
## ðŸŸ¡ Set up databricks free account in GCP Free Tier

## ðŸŸ¢ Dev Environment Setup
- **Create Dev Project in GCP** â†’ `project-dev-date`  
- **Create Dev Bucket in GCS** â†’ `bkt-dev-date`  
  - **Folders**  
    1. ðŸ“‚ **landing**  
       - raw_traffic  
       - raw_roads  
    2. ðŸ“‚ **checkpoints**  
    3. ðŸ“‚ **medallion**  
       - bronze  
       - silver  
       - gold  

- **Create Dev Workspace in Databricks** â†’ `dbx-dev-ws`  
- **Create Catalog** â†’ `dev_catalog`  
- **Create Dev Storage Credential** â†’ `bkt-dev-creds`  

- **Create External Locations** (5)  
  - landing_dev  
  - checkpoints_dev  
  - bronze_dev  
  - silver_dev  
  - gold_dev  

- **Create Databricks Cluster** â†’ `dev-cluster`  

---

## ðŸŸ¡ Schema & Table Creation
- **Schemas (01)**  
  - bronze  
  - silver  
  - gold  

- **Tables (02)**  
  - raw_roads  
  - raw_traffic  

---

## ðŸŸ  ETL Pipeline (Landing â†’ Medallion Architecture)

### Landing â†’ Bronze (03)
- Use **Autoloader in Batch Mode**  
- Upload sample data â†’ raw_traffic & raw_roads  
- Execute the notebook  
- Test by adding one more file and re-executing  

---

### Bronze â†’ Silver (04,05)
- Use **Structured Streaming in Batch Mode**  
- Execute the notebook  
- Test by adding one more file and re-executing  

---

### Commons Notebook
- Store **common variables**  
- Store **common functions** used in transformations  

---

### Silver â†’ Gold (06)
- Apply transformations as per reporting requirements  
- Write results to **Gold Tables**  

---

## ðŸ”µ Orchestration
- Update notebooks to job-ready format  
- Create **Job** â†’ `ETL Workflow`  
  - **Tasks**  
    - Task1 â†’ load_to_bronze (03)  
    - Task2 â†’ silver_traffic (04)  
    - Task3 â†’ silver_roads (05)  
    - Task4 â†’ gold (06)  

- **Add Trigger**  
  - File Arrival â†’ `landing/traffic` folder  
    - Notification for success/failure  
    - Upload sample file for testing  
  - Schedule â†’ For `roads` dataset (monthly updates)  

---

## ðŸŸ£ UAT Environment Setup
- **Create UAT Project in GCP** â†’ `project-uat-date`  
- **Create UAT Bucket in GCS** â†’ `bkt-uat-date`  
  - **Folders**  
    1. landing â†’ raw_traffic, raw_roads  
    2. checkpoints  
    3. medallion â†’ bronze, silver, gold  

- **Create UAT Workspace in Databricks** â†’ `gcp-dbx-uat`  
- **Create Catalog** â†’ `uat_catalog`  
- **Create Storage Credential** â†’ `bkt-uat-creds`  

- **Create External Locations (5)**  
  - landing_uat  
  - checkpoints_uat  
  - bronze_uat  
  - silver_uat  
  - gold_uat  

- **Create Databricks Cluster** â†’ `uat-cluster`  

---

## ðŸŸ¤ GitHub Integration
- Create repo â†’ `gcp-dbx-traffic`  
- Integrate GitHub with Databricks  
  - Settings â†’ Linked accounts â†’ Add Git Integration â†’ Sign-in  
  - Workspace â†’ Repos â†’ Add Git Folder â†’ `main` branch â†’ success  

- **Branching Strategy**  
  - Dev â†’ UAT â†’ PRD (main)  

---

## ðŸŸ¢ Dev â†’ UAT â†’ PRD Workflow
### Dev
- Development in **dev branch**  
- Clone project â†’ commit & push  
- Update pipeline with Git URL/branch + notebook path  
- Run & test pipeline  

### UAT
- Create PR â†’ merge (if no conflicts)  
- In UAT workspace â†’ add Git folder â†’ switch to `uat` branch  
- Execute codes with parameter `(uat)`  
- Create ETL workflow using Git notebooks  

### PRD
- Repeat same process for **prd** environment  

---
